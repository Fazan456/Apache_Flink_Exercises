{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "\n",
    "def create_kinesis_stream(stream_name, shard_count):\n",
    "    try:\n",
    "        # Initialize the Kinesis client\n",
    "        kinesis_client = boto3.client('kinesis')\n",
    "\n",
    "        # Create the Kinesis stream\n",
    "        response = kinesis_client.create_stream(\n",
    "            StreamName=stream_name,\n",
    "            ShardCount=shard_count\n",
    "        )\n",
    "\n",
    "        # Check for successful response\n",
    "        if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            print(f\"Kinesis stream '{stream_name}' created with {shard_count} shard(s)\")\n",
    "        else:\n",
    "            print(\"Failed to create Kinesis stream\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "def delete_kinesis_stream(stream_name):\n",
    "    try:\n",
    "        # Initialize the Kinesis client\n",
    "        kinesis_client = boto3.client('kinesis')\n",
    "\n",
    "        # Delete the Kinesis stream\n",
    "        response = kinesis_client.delete_stream(\n",
    "            StreamName=stream_name\n",
    "        )\n",
    "\n",
    "        # Check for successful response\n",
    "        if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            print(f\"Kinesis stream '{stream_name}' deleted successfully\")\n",
    "        else:\n",
    "            print(\"Failed to delete Kinesis stream\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_kinesis_stream(\"input-stream\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 2 : Publish some Dummy Data into Input Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import boto3\n",
    "\n",
    "STREAM_NAME = \"input-stream\"\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    return {\n",
    "        'event_time': datetime.datetime.now().isoformat(),\n",
    "        'ticker': random.choice(['AAPL', 'AMZN', 'MSFT', 'INTC', 'TBV']),\n",
    "        'price': round(random.random() * 100, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "def generate(stream_name, kinesis_client, num_samples):\n",
    "    for _ in range(num_samples):\n",
    "        data = get_data()\n",
    "        print(data)\n",
    "        kinesis_client.put_record(\n",
    "            StreamName=stream_name,\n",
    "            Data=json.dumps(data),\n",
    "            PartitionKey=\"partitionkey\")\n",
    "if __name__ == '__main__':\n",
    "    num_samples = 10  # Change this to the desired number of samples\n",
    "    generate(STREAM_NAME, boto3.client('kinesis'), num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Python Flink code\n",
    "# Download JAR file flink-sql-connector-kinesis-1.16.1.jar and create file\n",
    "# Download JAR  file flink-sql-connector-kinesis-1.16.1.jar and create file : \n",
    "https://mvnrepository.com/artifact/org.apache.flink/flink-sql-connector-kinesis/1.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Code Explanation\n",
    "\n",
    "This Python script demonstrates a PyFlink streaming application that processes data from a Kinesis Data Stream. Below are the key points explaining how the script works:\n",
    "\n",
    "    It imports the necessary libraries, including PyFlink and os.\n",
    "    It creates a PyFlink Table Environment (table_env) for stream processing.\n",
    "    Configuration properties are defined in the props list. These properties include the Python script to execute and the JAR file for the Flink-Kinesis connector.\n",
    "    The property_map function is used to extract property maps from the configuration properties.\n",
    "    The create_source_table function generates a SQL definition for a source table that reads data from a Kinesis Data Stream.\n",
    "\n",
    "In the main function:\n",
    "\n",
    "    Application properties are extracted from the props list.\n",
    "    The current working directory is obtained and used to set the JAR file path.\n",
    "    A source table is created using the create_source_table function.\n",
    "    Data is read from the source table and printed to the console.\n",
    "    Depending on whether the environment variable IS_LOCAL is set, the Flink job is executed either synchronously or asynchronously, and the job status is printed.\n",
    "\n",
    "Finally, the script is executed when run as the main program.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyflink.table import EnvironmentSettings, TableEnvironment\n",
    "import os\n",
    "\n",
    "# 1. Create a Table Environment\n",
    "env_settings = EnvironmentSettings.in_streaming_mode()\n",
    "table_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "# Define configuration properties for the Flink job\n",
    "props = [\n",
    "    {\n",
    "        \"PropertyGroupId\": \"kinesis.analytics.flink.run.options\",\n",
    "        \"PropertyMap\": {\n",
    "            \"python\": \"GettingStarted/getting-started.py\",  # Python script to be executed\n",
    "            \"jarfile\": \"flink-sql-connector-kinesis-1.16.1.jar\"  # Flink connector JAR file\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"PropertyGroupId\": \"consumer.config.0\",\n",
    "        \"PropertyMap\": {\n",
    "            \"input.stream.name\": \"input-stream\",        # Name of the Kinesis Data Stream\n",
    "            \"flink.stream.initpos\": \"TRIM_HORIZON\",     # Initial position to start reading the stream\n",
    "            \"aws.region\": \"us-east-1\"                   # AWS region where the stream is located\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to extract property map from the configuration properties\n",
    "def property_map(props, property_group_id):\n",
    "    for prop in props:\n",
    "        if prop[\"PropertyGroupId\"] == property_group_id:\n",
    "            return prop[\"PropertyMap\"]\n",
    "\n",
    "# Function to create a source table definition for Kinesis stream\n",
    "def create_source_table(table_name, stream_name, region, stream_initpos):\n",
    "    return f\"\"\" CREATE TABLE {table_name} (\n",
    "                ticker VARCHAR(6),\n",
    "                price DOUBLE,\n",
    "                event_time TIMESTAMP(3),\n",
    "                WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND\n",
    "              )\n",
    "              PARTITIONED BY (ticker)\n",
    "              WITH (\n",
    "                'connector' = 'kinesis',\n",
    "                'stream' = '{stream_name}',\n",
    "                'aws.region' = '{region}',\n",
    "                'scan.stream.initpos' = '{stream_initpos}',\n",
    "                'format' = 'json',\n",
    "                'json.timestamp-format.standard' = 'ISO-8601'\n",
    "              ) \"\"\"\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Define keys for accessing properties\n",
    "    input_property_group_key = \"consumer.config.0\"\n",
    "    input_stream_key = \"input.stream.name\"\n",
    "    input_region_key = \"aws.region\"\n",
    "    input_starting_position_key = \"flink.stream.initpos\"\n",
    "\n",
    "    # Table name for the source data\n",
    "    input_table_name = \"input_table\"\n",
    "\n",
    "    # Get application properties from the props list\n",
    "    input_property_map = property_map(props, input_property_group_key)\n",
    "    input_stream = input_property_map[input_stream_key]\n",
    "    input_region = input_property_map[input_region_key]\n",
    "    stream_initpos = input_property_map[input_starting_position_key]\n",
    "\n",
    "    # Check if running in a local environment and set pipeline.jars configuration\n",
    "    CURRENT_DIR = os.getcwd()  # Get the current working directory\n",
    "    table_env.get_config().get_configuration().set_string(\n",
    "        \"pipeline.jars\",\n",
    "        \"file:///\" + CURRENT_DIR + \"/flink-sql-connector-kinesis-1.16.1.jar\",\n",
    "    )\n",
    "\n",
    "    # 2. Create a source table from a Kinesis Data Stream\n",
    "    table_env.execute_sql(\n",
    "        create_source_table(input_table_name, input_stream, input_region, stream_initpos)\n",
    "    )\n",
    "\n",
    "    print(\"Successfully created source tables \")\n",
    "\n",
    "    # 3. Read data from the source table and print it to the console\n",
    "    query = f\"SELECT * FROM {input_table_name}\"\n",
    "    print(f\"Executing query: {query}\")\n",
    "    table_env.execute_sql(query).print()\n",
    "\n",
    "    if os.environ.get(\"IS_LOCAL\"):\n",
    "        # In a local environment, execute the job synchronously\n",
    "        table_env.execute(\"Getting Started Job\")\n",
    "    else:\n",
    "        # In a remote environment, execute the job asynchronously and print the job status\n",
    "        job_client = table_env.execute_async(\"Getting Started Job\")\n",
    "        print(job_client.get_job_status())\n",
    "\n",
    "# Entry point for the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up : Delete Kinesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_kinesis_stream(\"input-stream\")\n",
    "delete_kinesis_stream(\"output-stream\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
